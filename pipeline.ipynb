{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pyspark.sql.functions as sf\n",
    "# from uuid import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col\n",
    "# from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from uuid import UUID\n",
    "import time_uuid\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/23 02:27:33 WARN Utils: Your hostname, tester resolves to a loopback address: 127.0.1.1; using 172.25.162.57 instead (on interface eth0)\n",
      "23/08/23 02:27:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/yourname/miniconda3/envs/sparky/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-37b9b9aa-eb01-4cc4-94b1-81a9c3238354;1.0\n",
      "\tconfs: [default]\n",
      "\tfound mysql#mysql-connector-java;8.0.30 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.1.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.1.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.12.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.12.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.12.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 4413ms :: artifacts dl 938ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.12.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.12.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.12.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.1.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.1.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.30 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   0   |   0   |   0   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-37b9b9aa-eb01-4cc4-94b1-81a9c3238354\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/253ms)\n",
      "23/08/23 02:27:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "packages = [\"mysql:mysql-connector-java:8.0.30\",\n",
    "\"com.datastax.spark:spark-cassandra-connector_2.12:3.1.0\"]\n",
    "packages_str = ','.join(packages)\n",
    "# .config(\"spark.jars.packages\", packages_str) \\\n",
    "# khởi tạo session\n",
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.jars.packages\", packages_str) \\\n",
    ".config(\"spark.cassandra.connection.host\", \"localhost\")\\\n",
    ".config(\"spark.cassandra.connection.port\", \"9042\").getOrCreate()\n",
    "\n",
    "    # Retrieve the SparkConf object from the SparkContext\n",
    "#conf = spark.sparkContext.getConf()\n",
    "#conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def extract_time(uuid): # convert uuid to datetime \n",
    "  my_uuid =  UUID(uuid)\n",
    "  return time_uuid.TimeUUID(bytes=my_uuid.bytes).get_datetime().strftime(\"%Y-%m-%d %H:%M:%S\") #2022-07-26 06:45:09\n",
    "\n",
    "def process_df(df): # convert uuuid từ cột creatime thay thế cho cột ts\n",
    "# Spilt data\n",
    "\n",
    "\n",
    "  # Cast data String to Int\n",
    "  df = df.withColumns({'job_id': col('job_id').cast('int'),\n",
    "                      'bid': col('bid').cast('int'),\n",
    "                      'group_id': col('group_id').cast('int'),\n",
    "                      'publisher_id': col('publisher_id').cast('int'),\n",
    "                      'campaign_id': col('campaign_id').cast('int'),})\n",
    "  # Filter null data\n",
    "  df = df.filter(df.job_id.isNotNull()) # nên sử dụng cách này, có tác dụng khi job_id là Int\n",
    "  # df = df.filter(col('job_id') !='null') # khắc phục tạm vì do lúc lưu data gặp lỗi nên job_id là String\n",
    "\n",
    "  # Converting function to UDF\n",
    "  # StringType() is by default hence not required\n",
    "  extract_timeUDF = udf(lambda x: extract_time(x), StringType())\n",
    "\n",
    "  df = df.withColumn('ts', extract_timeUDF(col('create_time')))\n",
    "  \n",
    "  df = df.select('ts','job_id','custom_track','bid','campaign_id','group_id','publisher_id')\n",
    "\n",
    "  return df\n",
    "  # Cách 2: Thay vì dùng UDF có thể collect() cột create-time ra rồi duyệt vòng for đưa vào 1 list mới\n",
    "  # Cách 3: sử dụng apply(lambda:........)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_clicks(df):\n",
    "    clicks_data = df.filter(col('custom_track') == \"click\")\n",
    "    clicks_data = clicks_data.na.fill({'bid':0})\n",
    "    clicks_data = clicks_data.na.fill({'campaign_id':0})\n",
    "    clicks_data = clicks_data.na.fill({'group_id':0})\n",
    "    clicks_data = clicks_data.na.fill({'publisher_id':0})\n",
    "    clicks_data.registerTempTable('clicks') # Tạo bảng tạm để thực hiện truy vấn spark sql\n",
    "    clicks_output = spark.sql(\"\"\" SELECT  job_id, date(ts) AS dates, hour(ts) AS hours, publisher_id, campaign_id, group_id,\n",
    "                      avg(bid) AS bid_set, count(*) AS clicks, sum(bid) AS spend_hour\n",
    "        FROM clicks\n",
    "        GROUP BY job_id, date(ts) , hour(ts) , publisher_id, campaign_id, group_id \"\"\")\n",
    "    return clicks_output # return a SparkDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_qualified(df):\n",
    "    qualified_data = df.filter(col('custom_track') == \"qualified\")\n",
    "    qualified_data = qualified_data.na.fill({'bid':0})\n",
    "    qualified_data = qualified_data.na.fill({'campaign_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'group_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'publisher_id':0})\n",
    "    qualified_data.registerTempTable('qualified') # Tạo bảng tạm để thực hiện truy vấn spark sql\n",
    "    qualified_output = spark.sql(\"\"\" SELECT  job_id, date(ts) AS dates, hour(ts) AS hours, publisher_id, campaign_id, group_id,\n",
    "                      count(*) AS qualified\n",
    "        FROM qualified\n",
    "        GROUP BY job_id, date(ts) , hour(ts) ,publisher_id, campaign_id, group_id \"\"\")\n",
    "    return qualified_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_final_data(clicks_output,qualified_output):\n",
    "  final_data = clicks_output.join(qualified_output, ['job_id', 'dates', 'hours','publisher_id', 'campaign_id', 'group_id'],'full')\n",
    "  return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_output(df):\n",
    "    clicks = calculating_clicks(df)\n",
    "    qualified = calculating_qualified(df)\n",
    "    return process_final_data(clicks, qualified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_to_sql(output):\n",
    "    final_output = output.select('job_id','dates','hours','publisher_id','campaign_id','group_id','qualified','clicks','bid_set','spend_hour') # xếp lại thứ tự\n",
    "    \n",
    "    final_output.write.mode(\"append\")\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\",  \"jdbc:mysql://localhost:3306/ware\")\\\n",
    "    .option(\"dbtable\", \"events\")\\\n",
    "    .option(\"user\",  \"root\")\\\n",
    "    .option(\"password\",\"123456\")\\\n",
    "    .option(\"driver\",'com.mysql.cj.jdbc.Driver')\\\n",
    "    .save()\n",
    "    \n",
    "    return print('Data imported successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_mysql_lastest_time(url, driver, user, password):\n",
    "    sql = \"\"\"(select max(last_updated_time) from events) A\"\"\"\n",
    "    mysql_time = spark.read.format('jdbc').options(url = url , driver = driver , dbtable = sql, user=user , password = password).load().take(1)[0][0]\n",
    "    if mysql_time is None:\n",
    "        time = '1998-01-01 23:59:59'\n",
    "    else:\n",
    "        time = mysql_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_cass_lastest_time():\n",
    "    temp_df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"tracking\",keyspace=\"test\").load() # read Data\n",
    "    cass_time = temp_df.agg({'ts':'max'}).take(1)[0][0]\n",
    "\n",
    "    return cass_time.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mysql_time):\n",
    "    \n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Retrieve data from Cassandra and process\")\n",
    "    print(\"-----------------------------\")\n",
    "    df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"tracking\",keyspace=\"test\").load().where(col('ts') >= mysql_time) # read Data\n",
    "    \n",
    "    print(\"-----------------------------\")\n",
    "    print(\"process data\")\n",
    "    print(\"-----------------------------\")\n",
    "    df = process_df(df)\n",
    "\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"process final output\")\n",
    "    print(\"-----------------------------\")\n",
    "    cass_output = final_output(df)\n",
    "    \n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Process data successfully\")\n",
    "    print(\"-----------------------------\")\n",
    "\n",
    "    import_to_sql(cass_output)\n",
    "    return 'Task successfully'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest time in MySQL is 2023-08-23 00:07:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 02:30:19 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest time in Cassandra is 2023-08-23 02:28:46\n",
      "-----------------------------\n",
      "Retrieve data from Cassandra and process\n",
      "-----------------------------\n",
      "-----------------------------\n",
      "process data\n",
      "-----------------------------\n",
      "-----------------------------\n",
      "process final output\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yourname/miniconda3/envs/sparky/lib/python3.10/site-packages/pyspark/sql/dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n",
      "23/08/23 02:30:32 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Process data successfully\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 02:30:45 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "23/08/23 02:30:45 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully\n",
      "Job takes 115.218002 seconds to execute\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest time in MySQL is 2023-08-23 02:32:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 02:32:44 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest time in Cassandra is 2023-08-23 02:32:15\n",
      "-----------------------------\n",
      "Retrieve data from Cassandra and process\n",
      "-----------------------------\n",
      "-----------------------------\n",
      "process data\n",
      "-----------------------------\n",
      "-----------------------------\n",
      "process final output\n",
      "-----------------------------\n",
      "-----------------------------\n",
      "Process data successfully\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 02:32:52 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "23/08/23 02:32:52 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully\n",
      "Job takes 29.131845 seconds to execute\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mJob takes \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m seconds to execute\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat((timeend \u001b[39m-\u001b[39m timestart)\u001b[39m.\u001b[39mtotal_seconds()))\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-------------------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m30\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url=   \"jdbc:mysql://localhost:3306/ware\"\n",
    "driver = \"com.mysql.cj.jdbc.Driver\"\n",
    "user = 'root'\n",
    "password = '123456'\n",
    "\n",
    "while True:\n",
    "    timestart = datetime.datetime.now()\n",
    "\n",
    "    mysql_time = retrieve_mysql_lastest_time(url, driver, user, password)\n",
    "    print(f'Latest time in MySQL is {mysql_time}')\n",
    "\n",
    "    cassandra_time = retrieve_cass_lastest_time()\n",
    "    print(f'Latest time in Cassandra is {cassandra_time}')\n",
    "\n",
    "    if cassandra_time > mysql_time :\n",
    "        main(mysql_time)\n",
    "    else :\n",
    "        print('No New data found')\n",
    "\n",
    "    timeend = datetime.datetime.now()\n",
    "    \n",
    "    print(\"Job takes {} seconds to execute\".format((timeend - timestart).total_seconds()))\n",
    "\n",
    "    print(\"-------------------------------------------\")\n",
    "    time.sleep(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparky",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
